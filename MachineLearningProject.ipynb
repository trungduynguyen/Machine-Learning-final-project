{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: Red wine quality Data Analysis\n",
    "## Trung Duy .N Nguyen, ICT2015\n",
    "### John von Neumann Institute\n",
    "#### November $30^{th}$, 2015\n",
    "\n",
    "\n",
    "Content\n",
    "=========\n",
    "\n",
    "1. Dataset description\n",
    "2. Objective \n",
    "3. The experiment\n",
    "4. Conclusion\n",
    "\n",
    "\n",
    "I. Dataset description\n",
    "=========\n",
    "\n",
    "Name of the dataset: **Wine Quality**\n",
    "\n",
    "Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n",
    "\n",
    "Source: [https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/]\n",
    "\n",
    "Description: The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n",
    "  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n",
    "  between 0 (very bad) and 10 (very excellent).These datasets can be viewed as classification or regression tasks.\n",
    "  \n",
    "   Several data mining methods were applied to model these datasets under a regression approach.\n",
    "   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n",
    "   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n",
    "   or poor wines. Also, we are not sure if all input variables are relevant. So\n",
    "   it could be interesting to test feature selection methods. \n",
    "   \n",
    "Samples size: 1599 observations\n",
    "\n",
    "Description of attributes:\n",
    "\n",
    "1. **Fixed acidity**: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n",
    "2. **Volatile acidity**: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
    "3. **Citric acid**: found in small quantities, citric acid can add ‘freshness’ and flavor to wines\n",
    "4. **Residual sugar**: the amount of sugar remaining after fermentation stops, it’s rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
    "5. **Chlorides**: the amount of salt in the wine\n",
    "6. **Free sulfur dioxide**: the free form of $SO_{2}$ exists in equilibrium between molecular $SO_{2}$ (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
    "7. **Total sulfur dioxide**: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, $SO_{2}$ becomes evident in the nose and taste of wine\n",
    "8. **Density**: the density of water is close to that of water depending on the percent alcohol and sugar content\n",
    "9. **pH**: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n",
    "10. **Sulphates**: a wine additive which can contribute to sulfur dioxide gas ($S0_{2}$) levels, wich acts as an antimicrobial and antioxidant\n",
    "11. **Alcohol**: the percent alcohol content of the wine\n",
    "12. **Quality**: output variable (based on sensory data, score between 0 and 10)\n",
    "\n",
    "\n",
    "II. Objective\n",
    "=========\n",
    "\n",
    "As the requirements of the project, I follow 5 steps of the KDD process to show the process range from choosing dataset to evaluation models. In this project, the problem that I chose to solve is classification; thus, the Wine dataset will be seperated into binary classes .Each step will be describe in details\n",
    "\n",
    "** 1. Data preprocessing**\n",
    "-  Change the problem from regression to classification\n",
    "-  Data Splitting\n",
    "-  Correlation matrix (reduce dimensionals of features)\n",
    "-  Normalize data (change the domain of features)\n",
    "-  Resampling data\n",
    "\n",
    "** 2. Model Selection**\n",
    "\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors\n",
    "- Random Forests\n",
    "- Tunning Parameters ( only Random Forests)\n",
    "\n",
    "** 3. Evaluation**: Compare the accuracy of 3 chosen models\n",
    "\n",
    "** 4. Conclusion**: Draw the subjective conclusion about the characteristics of the data, and also verify what important features which affect the result of the models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. The experiment\n",
    "=========\n",
    "\n",
    "**1. Load required packages**\n",
    "\n",
    "In this project, \"caret\" package is used as a main library which means that many functions and Machine Learning models are invoked through all the steps. Beside, I also include some graphical libraries for the use of demonstration figures and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Loading required package: MASS\n",
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(caret)         # classification and regression training\n",
    "library(corrplot)      # graphical display of the correlation matrix\n",
    "library(class)         # K-nearest neighbors\n",
    "library(klaR)          # naive bayes\n",
    "library(randomForest)  # Random Forests\n",
    "library(gridExtra)     # save dataframes as images\n",
    "library(pROC)\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "today <- as.character(Sys.Date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>fixed.acidity</th><th scope=col>volatile.acidity</th><th scope=col>citric.acid</th><th scope=col>residual.sugar</th><th scope=col>chlorides</th><th scope=col>free.sulfur.dioxide</th><th scope=col>total.sulfur.dioxide</th><th scope=col>density</th><th scope=col>pH</th><th scope=col>sulphates</th><th scope=col>alcohol</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>7.4</td><td>0.7</td><td>0</td><td>1.9</td><td>0.076</td><td>11</td><td>34</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>7.8</td><td>0.88</td><td>0</td><td>2.6</td><td>0.098</td><td>25</td><td>67</td><td>0.9968</td><td>3.2</td><td>0.68</td><td>9.8</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7.8</td><td>0.76</td><td>0.04</td><td>2.3</td><td>0.092</td><td>15</td><td>54</td><td>0.997</td><td>3.26</td><td>0.65</td><td>9.8</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>11.2</td><td>0.28</td><td>0.56</td><td>1.9</td><td>0.075</td><td>17</td><td>60</td><td>0.998</td><td>3.16</td><td>0.58</td><td>9.8</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>7.4</td><td>0.7</td><td>0</td><td>1.9</td><td>0.076</td><td>11</td><td>34</td><td>0.9978</td><td>3.51</td><td>0.56</td><td>9.4</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & fixed.acidity & volatile.acidity & citric.acid & residual.sugar & chlorides & free.sulfur.dioxide & total.sulfur.dioxide & density & pH & sulphates & alcohol\\\\\n",
       "\\hline\n",
       "\t1 & 7.4 & 0.7 & 0 & 1.9 & 0.076 & 11 & 34 & 0.9978 & 3.51 & 0.56 & 9.4\\\\\n",
       "\t2 & 7.8 & 0.88 & 0 & 2.6 & 0.098 & 25 & 67 & 0.9968 & 3.2 & 0.68 & 9.8\\\\\n",
       "\t3 & 7.8 & 0.76 & 0.04 & 2.3 & 0.092 & 15 & 54 & 0.997 & 3.26 & 0.65 & 9.8\\\\\n",
       "\t4 & 11.2 & 0.28 & 0.56 & 1.9 & 0.075 & 17 & 60 & 0.998 & 3.16 & 0.58 & 9.8\\\\\n",
       "\t5 & 7.4 & 0.7 & 0 & 1.9 & 0.076 & 11 & 34 & 0.9978 & 3.51 & 0.56 & 9.4\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n",
       "1           7.4             0.70        0.00            1.9     0.076\n",
       "2           7.8             0.88        0.00            2.6     0.098\n",
       "3           7.8             0.76        0.04            2.3     0.092\n",
       "4          11.2             0.28        0.56            1.9     0.075\n",
       "5           7.4             0.70        0.00            1.9     0.076\n",
       "  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n",
       "1                  11                   34  0.9978 3.51      0.56     9.4\n",
       "2                  25                   67  0.9968 3.20      0.68     9.8\n",
       "3                  15                   54  0.9970 3.26      0.65     9.8\n",
       "4                  17                   60  0.9980 3.16      0.58     9.8\n",
       "5                  11                   34  0.9978 3.51      0.56     9.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine <- read.csv('winequality-red.csv',sep = ',')\n",
    "head(wine[1:11],5)\n",
    "wine$quality <- as.integer(wine$quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data Preprocessing**\n",
    "\n",
    "**a. Change the problem from regression to classification**\n",
    "\n",
    "As the dataset description, the quality attribute is marked by the experts on the scale of 10 which is mumerical; therefore, this attribute is appropriate for regression purposes. However, I changed the characteristic of the feature to nominal. The method is grouping all instances with the quality are greater than 6 to the \"good\" class; the others, which means less than or equal 6, \"bad\" class. Thus, the new modified dataset is ready for classification tasks with 2 classes \"good\" and \"bad\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>bad</li>\n",
       "\t<li>bad</li>\n",
       "\t<li>bad</li>\n",
       "\t<li>good</li>\n",
       "\t<li>bad</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item bad\n",
       "\\item bad\n",
       "\\item bad\n",
       "\\item good\n",
       "\\item bad\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. bad\n",
       "2. bad\n",
       "3. bad\n",
       "4. good\n",
       "5. bad\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] bad  bad  bad  good bad \n",
       "Levels: bad good"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the problem from regression to classification\n",
    "good <- wine$quality >= 6\n",
    "bad <- wine$quality < 6\n",
    "wine[good, 'quality'] <- 'good'\n",
    "wine[bad, 'quality'] <- 'bad'  \n",
    "wine$quality <- as.factor(wine$quality) # redefine the factor variable\n",
    "\n",
    "dummies <- dummyVars(quality ~ ., data = wine)\n",
    "wine_dummied <- data.frame(predict(dummies, newdata = wine))\n",
    "wine_dummied[, 'quality'] <- wine$quality\n",
    "\n",
    "head(wine$quality, 5) # now, the quality feature is nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**b. Data Splitting**\n",
    "\n",
    "I use the cross validation method to split the data into train set and test set with the proportion is 0.7 for train and 0.3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "trainIndex <- createDataPartition(wine$quality, p = .7,list = FALSE,times = 1)\n",
    "wineTrain <- wine_dummied[ trainIndex,]\n",
    "wineTest  <- wine_dummied[-trainIndex,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Correlation matrix**\n",
    "\n",
    "The purpose of finding the corellation matrix of the dataset is to get rid of the linearity in the data, which means reducing numbers of features are highly correlated, so that leads to the decreasing of the complexity when we apply machine learning models to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>\"citric.acid\"</li>\n",
       "\t<li>\"fixed.acidity\"</li>\n",
       "\t<li>\"total.sulfur.dioxide\"</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item \"citric.acid\"\n",
       "\\item \"fixed.acidity\"\n",
       "\\item \"total.sulfur.dioxide\"\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. \"citric.acid\"\n",
       "2. \"fixed.acidity\"\n",
       "3. \"total.sulfur.dioxide\"\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"citric.acid\"          \"fixed.acidity\"        \"total.sulfur.dioxide\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Corelation matrix\n",
    "numericCol <- !colnames(wineTrain) %in% c('quality')\n",
    "correlMatrix <- cor(wineTrain[, numericCol])\n",
    "highlyCorrelated <- findCorrelation(correlMatrix, cutoff = 0.6) # features are highly correlated if threshold > 0.6 \n",
    "colnames(correlMatrix)[highlyCorrelated]\n",
    "\n",
    "png(paste0(today, '-', 'correlation-matrix of 11 features.png'))\n",
    "corrplot(correlMatrix, method = 'number', tl.cex = 0.5)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2015-11-30-correlation-matrix of 11 features.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the matrix has shown us, it is obvious to see that total.sulfur.dioxide has linearly correlated with other variables; therefore, I am not going not include this feature in any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove total.sulfur.dioxide from data.frame\n",
    "wineTrain <- wineTrain[ , -which(names(wineTrain) %in% c(\"total.sulfur.dioxide\"))]\n",
    "wineTest <- wineTest[ , -which(names(wineTest) %in% c(\"total.sulfur.dioxide\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. Normalize data **\n",
    "\n",
    "In the \"caret\" package, I use the function preProcess to normalize all the features in the dataset (from the $1^{st}$ column to the $11^{th}$ column ) with the method \"range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normalized <- preProcess(wineTrain[, 1:10], method = 'range')\n",
    "train_plot <- predict(train_normalized, wineTrain[, 1:10])\n",
    "png(paste0(today, '-', 'feature-plot.png'))\n",
    "featurePlot(train_plot, wineTrain$quality, 'box')\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"2015-11-30-feature-plot.PNG\">\n",
    "\n",
    "From the figure, it looks like alcohol, citric.acid and density separate most with regard to good classification. Then these 3 features will be included in Machine Learning models\n",
    "\n",
    "**e. Resampling data**\n",
    "\n",
    "The function trainControl in the caret package is a very comfortable way to set the resampling data we need. Among the resampling methods which accepts as argument, there are an extensive of choices such as bootstrap, cross validation, and repeated cross validation... In this project, I would like to demonstrate the resampling data's process by using cross validation method with the number of folds is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = 'cv', number = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Model Selection**\n",
    "\n",
    "In this section, I would like to apply 3 supervised learning models which I researched. There are Naive Bayes, K-Nearest Neighbor and Random Forests. Through every models, I present the way how we input parameters of the train data into a model in \"caret\" library. Then, the next step is predicting on the test dataset, which based on the result of train data. Afterall, the confusion matrix is constructed to compute the accuracy of the chosen model. Furthermore, I discover that \"caret\" library has provided us a function to calculate the level of importance of the attributes; therefore, I also include this function in the script and visualize it for better evaluation what important features of the model.\n",
    "\n",
    "**a. Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 29Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 44Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 44Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 48Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 74Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 98Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 6Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 100Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 112Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 10Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 25Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 9Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 104Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 98Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 99Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 5Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 24Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 31Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 56Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 98Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 99Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 15Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 15Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 86Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 93Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 80Warning message:\n",
      "In FUN(X[[i]], ...): Numerical 0 probability for all classes with observation 381"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction bad good\n",
       "      bad  171   75\n",
       "      good  52  181\n",
       "                                          \n",
       "               Accuracy : 0.7349          \n",
       "                 95% CI : (0.6929, 0.7739)\n",
       "    No Information Rate : 0.5344          \n",
       "    P-Value [Acc > NIR] : < 2e-16         \n",
       "                                          \n",
       "                  Kappa : 0.4707          \n",
       " Mcnemar's Test P-Value : 0.05092         \n",
       "                                          \n",
       "            Sensitivity : 0.7070          \n",
       "            Specificity : 0.7668          \n",
       "         Pos Pred Value : 0.7768          \n",
       "         Neg Pred Value : 0.6951          \n",
       "             Prevalence : 0.5344          \n",
       "         Detection Rate : 0.3779          \n",
       "   Detection Prevalence : 0.4864          \n",
       "      Balanced Accuracy : 0.7369          \n",
       "                                          \n",
       "       'Positive' Class : good            \n",
       "                                          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################\n",
    "###############NAIVEBAYES#####################\n",
    "##############################################\n",
    "fit_nb <- train(x = wineTrain[, 1:10], y = wineTrain$quality,\n",
    "                method ='nb',trControl = fitControl)\n",
    "predict_nb <- predict(fit_nb, newdata = wineTest[, 1:10])\n",
    "confMat_nb <- confusionMatrix(predict_nb, wineTest$quality, positive = 'good')\n",
    "importance_nb <- varImp(fit_nb, scale = TRUE)\n",
    "\n",
    "confMat_nb\n",
    "\n",
    "png(paste0(today, '-', 'importance-nb.png'))\n",
    "plot(importance_nb, main = 'Feature importance for Naive Bayes')\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. K-Nearest Neighbor **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction bad good\n",
       "      bad  158   71\n",
       "      good  65  185\n",
       "                                          \n",
       "               Accuracy : 0.7161          \n",
       "                 95% CI : (0.6734, 0.7561)\n",
       "    No Information Rate : 0.5344          \n",
       "    P-Value [Acc > NIR] : 3.124e-16       \n",
       "                                          \n",
       "                  Kappa : 0.4304          \n",
       " Mcnemar's Test P-Value : 0.6681          \n",
       "                                          \n",
       "            Sensitivity : 0.7227          \n",
       "            Specificity : 0.7085          \n",
       "         Pos Pred Value : 0.7400          \n",
       "         Neg Pred Value : 0.6900          \n",
       "             Prevalence : 0.5344          \n",
       "         Detection Rate : 0.3862          \n",
       "   Detection Prevalence : 0.5219          \n",
       "      Balanced Accuracy : 0.7156          \n",
       "                                          \n",
       "       'Positive' Class : good            \n",
       "                                          "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################\n",
    "#####################KNN######################\n",
    "##############################################\n",
    "fit_knn <- train(x = wineTrain[, 1:10], y = wineTrain$quality,\n",
    "                 method = 'knn',\n",
    "                 preProcess = 'range', \n",
    "                 trControl = fitControl, \n",
    "                 tuneGrid = expand.grid(.k = \n",
    "                          c(3, 5, 7, 9, 11, 15, 21, 25, 31, 41, 51, 75, 101)))  \n",
    "predict_knn <- predict(fit_knn, newdata = wineTest[, 1:10])\n",
    "confMat_knn <- confusionMatrix(predict_knn, wineTest$quality, positive = 'good')\n",
    "confMat_knn\n",
    "\n",
    "importance_knn <- varImp(fit_knn, scale = TRUE)\n",
    "\n",
    "png(paste0(today, '-', 'importance-knn.png'))\n",
    "plot(importance_knn, main = 'Feature importance for K-nearest neighbor')\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Random Forests **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction bad good\n",
       "      bad  172   49\n",
       "      good  51  207\n",
       "                                         \n",
       "               Accuracy : 0.7912         \n",
       "                 95% CI : (0.752, 0.8268)\n",
       "    No Information Rate : 0.5344         \n",
       "    P-Value [Acc > NIR] : <2e-16         \n",
       "                                         \n",
       "                  Kappa : 0.5802         \n",
       " Mcnemar's Test P-Value : 0.9203         \n",
       "                                         \n",
       "            Sensitivity : 0.8086         \n",
       "            Specificity : 0.7713         \n",
       "         Pos Pred Value : 0.8023         \n",
       "         Neg Pred Value : 0.7783         \n",
       "             Prevalence : 0.5344         \n",
       "         Detection Rate : 0.4322         \n",
       "   Detection Prevalence : 0.5386         \n",
       "      Balanced Accuracy : 0.7899         \n",
       "                                         \n",
       "       'Positive' Class : good           \n",
       "                                         "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################\n",
    "#############RANDOMFORESTS####################\n",
    "##############################################\n",
    "\n",
    "fit_rf <- train(x = wineTrain[, 1:10], y = wineTrain$quality,\n",
    "                method = 'rf',\n",
    "                trControl = fitControl,\n",
    "                tuneGrid = expand.grid(.mtry = c(2:6)),\n",
    "                n.tree = 500) \n",
    "predict_rf <- predict(fit_rf, newdata = wineTest[, 1:10])\n",
    "confMat_rf <- confusionMatrix(predict_rf, wineTest$quality, positive = 'good')\n",
    "\n",
    "confMat_rf\n",
    "\n",
    "importance_rf <- varImp(fit_rf, scale = TRUE)\n",
    "\n",
    "png(paste0(today, '-', 'importance-rf.png'))\n",
    "plot(importance_rf, main = 'Feature importance for Random Forests')\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** d. Tunning parameters**\n",
    "\n",
    "Due to the lack of resources, I just try to tunning parameters in Random Forests. Thus, I choose the number of trees in the algorithm for the purpose to evaluate what it affect the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntree <- c(1, 30, 50, 80, 120, 150, 200, 300, 500, 550, 700) #Vector number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. Evaluate the accuracy of the three chosen model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models <- resamples(list(NB = fit_nb, KNN = fit_knn,\n",
    "                         RF = fit_rf))\n",
    "png(paste0(today, '-', 'models-comparison.png'))\n",
    "dotplot(models)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \" \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results <- summary(models)\n",
    "png(paste0(today, '-', 'models-accuracy.png'), width = 480, height = 180)\n",
    "grid.table(results$statistics$Accuracy)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \" \">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Conclusion\n",
    "=========\n",
    "It does not look like wine quality is well supported by its chemical properties ( We can easily see that in feature important of 3 chosen models). At each quality level variability of the predictors is high and the groups are not well separated.\n",
    "\n",
    "The total.sulfur.dioxide is highly correlated and should be excluded from any classifier.\n",
    "\n",
    "The acohol attribute strongly affects wines belonged to which class.\n",
    "\n",
    "Between 3 chosen models, Random Forests gives us the best result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
